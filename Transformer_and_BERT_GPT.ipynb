{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGxUK0cvPkpi"
      },
      "source": [
        "# Transformer로 감정 분류(Classification task) 수행하기\n",
        "\n",
        "Transformer는 분명, seq2seq 형태의 task에서 강점을 보입니다. 하지만, Attention과 Transformer를 classificatoin task에 적용할 수 있음\n",
        "\n",
        "Transformer의 Encoder 부분만을 활용하여 분류 태스크를 적용할 수 있으며, 전체 모델 그림은 다음과 같음"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZXkBVP8Vg4j"
      },
      "source": [
        "<img src=https://s3.ap-northeast-2.amazonaws.com/urclass-images/aoJFaVFMvRAsntP5k16yq-1640699853834.png>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdoConW05yG6"
      },
      "source": [
        "## Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDsXneJ_biSe"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUi1f898bjvm",
        "outputId": "8f283565-4076-4940-c3c3-0834173b3ebd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 1, 1, 5), dtype=float32, numpy=\n",
              "array([[[[0., 0., 0., 0., 1.]]],\n",
              "\n",
              "\n",
              "       [[[0., 0., 0., 1., 1.]]],\n",
              "\n",
              "\n",
              "       [[[0., 0., 1., 1., 1.]]]], dtype=float32)>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def create_padding_mask(seq):\n",
        "    # 패딩 처리된 부분을 마스킹합니다.\n",
        "    # 모델이 패딩을 입력으로 취급하지 않도록 \n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "x = tf.constant([[7, 6, 5, 4, 0], [1, 2, 3, 0, 0], [1, 8, 0, 0, 0]])\n",
        "create_padding_mask(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZzUGNtybdfc"
      },
      "outputs": [],
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "    \"\"\"\n",
        "    sin, cos 안에 들어갈 수치를 구하는 함수입니다.\n",
        "    \"\"\"\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    \"\"\"\n",
        "    위치 인코딩(Positional Encoding)을 구하는 함수입니다.\n",
        "    \"\"\"\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knLJBbXYExX9"
      },
      "source": [
        "### Dot Scaled Attention 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0O4AoOj2TCU6"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \n",
        "    \"\"\"Calculate the attention weights.\n",
        "    q, k, v must have matching leading dimensions.\n",
        "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "    The mask has different shapes depending on its type(padding or look ahead) \n",
        "    but it must be broadcastable for addition.\n",
        "  \n",
        "    Args:\n",
        "        q: query shape == (..., seq_len_q, depth)\n",
        "        k: key shape == (..., seq_len_k, depth)\n",
        "        v: value shape == (..., seq_len_v, depth_v)\n",
        "        mask: Float tensor with shape broadcastable \n",
        "            to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "    \n",
        "    Returns:\n",
        "        output, attention_weights\n",
        "    \"\"\"\n",
        "    \n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "    \n",
        "    # scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    \"\"\"\n",
        "    mask가 있을 경우 masking된 자리(mask=1)에는 (-inf)에 해당하는 절댓값이 큰 음수 -1e9(=-10억)을 더해줍니다.\n",
        "    그 값에 softmax를 취해주면 거의 0에 가까운 값이 나옵니다. 그 다음 value 계산시에 반영되지 않습니다.\n",
        "    \"\"\"\n",
        "    \n",
        "    # add the mask to the scaled tensor.\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)  \n",
        "        \n",
        "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "    # add up to 1.\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "    \n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "    \n",
        "    return output, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RsJgJm-Hl6T"
      },
      "source": [
        "### MultiHead Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsuAwzvwbgb1"
      },
      "outputs": [],
      "source": [
        "def point_wise_feed_forward_network(**kargs):\n",
        "    \"\"\"\n",
        "    FFNN을 구현한 코드입니다.\n",
        "\n",
        "    Args:\n",
        "        d_model : 모델의 차원입니다.\n",
        "        dff : 은닉층의 차원 수입니다. 논문에서는 2048을 사용하였습니다.\n",
        "    \"\"\"\n",
        "    return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(kargs['dff'], activation='relu'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(kargs['d_model'])  # (batch_size, seq_len, d_model)\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1fiUBlybbQM"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kargs):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = kargs['num_heads']\n",
        "        self.d_model = kargs['d_model']\n",
        "\n",
        "        assert self.d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = self.d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(units=self.d_model)\n",
        "        self.wk = tf.keras.layers.Dense(units=self.d_model)\n",
        "        self.wv = tf.keras.layers.Dense(units=self.d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(kargs['d_model'])\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"\n",
        "        Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention, \n",
        "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        return output, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IOF-NkAKOl9"
      },
      "source": [
        "### Encoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZY6JRIMYcAtC"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kargs):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(**kargs)\n",
        "        self.ffn = point_wise_feed_forward_network(**kargs)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(kargs['rate'])\n",
        "        self.dropout2 = tf.keras.layers.Dropout(kargs['rate'])\n",
        "\n",
        "    def call(self, x, mask):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        return out2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpMskzgBKR8q"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bm0eI4FcDFq"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = kwargs['d_model']\n",
        "        self.num_layers = kwargs['num_layers']\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(kwargs['vocab_size'], self.d_model)\n",
        "        self.pos_encoding = positional_encoding(kwargs['maximum_position_encoding'], \n",
        "                                                self.d_model)\n",
        "\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(**kwargs) \n",
        "                           for _ in range(self.num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(kwargs['rate'])\n",
        "\n",
        "    def call(self, x, mask):\n",
        "\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        # adding embedding and position encoding.\n",
        "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, mask)\n",
        "\n",
        "        return x  # (batch_size, input_seq_len, d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbFDQKh6MCT9"
      },
      "source": [
        "### Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pnb6bL72M1Yx"
      },
      "source": [
        "여기서는 가장 첫 번째 타임 스텝의 은닉 값을 가져와서 classification_layer에 통과시키기\n",
        "\n",
        "(batch_size, 1, d_model) 크기의 텐서가 바로 이진 분류를 수행하는 Dense Layer로 들어가게끔 구현\n",
        "- 이 부분은 설계에 따라 변경될 수 있으며, 다음과 같이 2개의 Dense Layer를 쌓아서 이진 분류를 수행할 수도 있음\n",
        "  - Dense(64, activation='relu')\n",
        "  - Dense(1, activation='sigmoid')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LjmMJg-wiqh"
      },
      "outputs": [],
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, **kargs):\n",
        "        super(Transformer, self).__init__()\n",
        "        \n",
        "        self.encoder = Encoder(**kargs)\n",
        "        self.classification_layer = tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        # Keras models prefer if you pass all your inputs in the first argument\n",
        "\n",
        "        enc_padding_mask = self.create_masks(inputs)\n",
        "\n",
        "        enc_output = self.encoder(inputs, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "\n",
        "        final_output = self.classification_layer(enc_output[:, 0, :]) # (bs, 1, d_model)\n",
        "\n",
        "        return final_output\n",
        "\n",
        "    def create_masks(self, inputs):\n",
        "        # Encoder padding mask\n",
        "        enc_padding_mask = create_padding_mask(inputs)\n",
        "\n",
        "        return enc_padding_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89RBkU8iMEzS"
      },
      "source": [
        "### Hyper-Parameter 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBQGLAWnvQpg"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 16\n",
        "MAX_SEQUENCE = 80\n",
        "EPOCHS = 3\n",
        "VALID_SPLIT = 0.1\n",
        "MAX_FEATURES = 20000\n",
        "MAXLEN = 80\n",
        "\n",
        "kargs = {'num_layers': 2,\n",
        "         'd_model': 512,\n",
        "         'num_heads': 8,\n",
        "         'dff': 2048,\n",
        "         'vocab_size': MAX_FEATURES,\n",
        "         'maximum_position_encoding': MAXLEN,\n",
        "         'rate': 0.1\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEdeHPWzMIuf"
      },
      "source": [
        "IMDB Review Data 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqLwHSR-tkZs",
        "outputId": "c242669f-0cf6-4726-90dc-787f399ec6d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "# 데이터를 불러옵니다.\n",
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=MAX_FEATURES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4w_U2Ajtt_v"
      },
      "outputs": [],
      "source": [
        "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=MAXLEN)\n",
        "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=MAXLEN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tegtzKWSMNT9"
      },
      "source": [
        "### 모델 선언"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_n4QWMruuyO7"
      },
      "outputs": [],
      "source": [
        "model = Transformer(**kargs)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics='accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oaHP-TVtSCM"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=10)\n",
        "\n",
        "checkpoint_path = 'weights.h5'\n",
        "\n",
        "cp_callback = ModelCheckpoint(\n",
        "    checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgYE15nHRulT"
      },
      "source": [
        "### 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lvz4wIJtuhv9",
        "outputId": "719e2443-287d-4f16-e3a4-f3c0c6b3bb6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "1406/1407 [============================>.] - ETA: 0s - loss: 0.5167 - accuracy: 0.7301\n",
            "Epoch 1: val_accuracy improved from -inf to 0.82680, saving model to weights.h5\n",
            "1407/1407 [==============================] - 49s 30ms/step - loss: 0.5166 - accuracy: 0.7301 - val_loss: 0.3757 - val_accuracy: 0.8268\n",
            "Epoch 2/3\n",
            "1405/1407 [============================>.] - ETA: 0s - loss: 0.3005 - accuracy: 0.8725\n",
            "Epoch 2: val_accuracy improved from 0.82680 to 0.83080, saving model to weights.h5\n",
            "1407/1407 [==============================] - 42s 30ms/step - loss: 0.3005 - accuracy: 0.8724 - val_loss: 0.3849 - val_accuracy: 0.8308\n",
            "Epoch 3/3\n",
            "1405/1407 [============================>.] - ETA: 0s - loss: 0.1719 - accuracy: 0.9348\n",
            "Epoch 3: val_accuracy did not improve from 0.83080\n",
            "1407/1407 [==============================] - 41s 29ms/step - loss: 0.1719 - accuracy: 0.9348 - val_loss: 0.4611 - val_accuracy: 0.8132\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(x_train, y_train, \n",
        "                    batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
        "                    validation_split=VALID_SPLIT, callbacks=[earlystop_callback, cp_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JyD-x3Ix_Rd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "n424a_Transformer_and_BERT_GPT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('s1n1')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "078b936573ee91fd7cfd3c845d42b329f9202605200ef02f5fc1ba2441611771"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

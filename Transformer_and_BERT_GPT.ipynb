{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEHN9J5Bwf5i"
      },
      "source": [
        "<img align=\"right\" src=\"https://ds-cs-images.s3.ap-northeast-2.amazonaws.com/Codestates_Fulllogo_Color.png\" width=100>\n",
        "\n",
        "## *AIB / SECTION 4 / SPRINT 2 / NOTE 4*\n",
        "\n",
        "# ğŸ“ Assignment\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Transformer_and_BERT_GPT"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformerë¡œ ê°ì • ë¶„ë¥˜(Classification task) ìˆ˜í–‰í•˜ê¸°\n",
        "\n",
        "TransformerëŠ” ë¶„ëª…, seq2seq í˜•íƒœì˜ taskì—ì„œ ê°•ì ì„ ë³´ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ, Attentionê³¼ Transformerë¥¼ classificatoin taskì— ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "Transformerì˜ Encoder ë¶€ë¶„ë§Œì„ í™œìš©í•˜ì—¬ ë¶„ë¥˜ íƒœìŠ¤í¬ë¥¼ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì „ì²´ ëª¨ë¸ ê·¸ë¦¼ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "gGxUK0cvPkpi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=https://s3.ap-northeast-2.amazonaws.com/urclass-images/aoJFaVFMvRAsntP5k16yq-1640699853834.png>"
      ],
      "metadata": {
        "id": "eZXkBVP8Vg4j"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdoConW05yG6"
      },
      "source": [
        "## Code"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "gDsXneJ_biSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_padding_mask(seq):\n",
        "    # íŒ¨ë”© ì²˜ë¦¬ëœ ë¶€ë¶„ì„ ë§ˆìŠ¤í‚¹í•©ë‹ˆë‹¤.\n",
        "    # ëª¨ë¸ì´ íŒ¨ë”©ì„ ì…ë ¥ìœ¼ë¡œ ì·¨ê¸‰í•˜ì§€ ì•Šë„ë¡ \n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "x = tf.constant([[7, 6, 5, 4, 0], [1, 2, 3, 0, 0], [1, 8, 0, 0, 0]])\n",
        "create_padding_mask(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUi1f898bjvm",
        "outputId": "8f283565-4076-4940-c3c3-0834173b3ebd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 1, 1, 5), dtype=float32, numpy=\n",
              "array([[[[0., 0., 0., 0., 1.]]],\n",
              "\n",
              "\n",
              "       [[[0., 0., 0., 1., 1.]]],\n",
              "\n",
              "\n",
              "       [[[0., 0., 1., 1., 1.]]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "    \"\"\"\n",
        "    sin, cos ì•ˆì— ë“¤ì–´ê°ˆ ìˆ˜ì¹˜ë¥¼ êµ¬í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    \"\"\"\n",
        "    ìœ„ì¹˜ ì¸ì½”ë”©(Positional Encoding)ì„ êµ¬í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "metadata": {
        "id": "oZzUGNtybdfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dot Scaled Attention êµ¬í˜„"
      ],
      "metadata": {
        "id": "knLJBbXYExX9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ë¬¸ì œ1"
      ],
      "metadata": {
        "id": "nXEANS8fGbLL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtSLndjKTCU6"
      },
      "source": [
        "`\"\"\"input your code\"\"\"` ë¶€ë¶„ì— ì•Œë§ì€ ì½”ë“œë¡œ ì•Œë§ê²Œ ì§ì§€ì–´ì§„ ê²ƒì„ ê³ ë¥´ì‹œì˜¤. (A, B, C)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0O4AoOj2TCU6"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \n",
        "    \"\"\"Calculate the attention weights.\n",
        "    q, k, v must have matching leading dimensions.\n",
        "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "    The mask has different shapes depending on its type(padding or look ahead) \n",
        "    but it must be broadcastable for addition.\n",
        "  \n",
        "    Args:\n",
        "        q: query shape == (..., seq_len_q, depth)\n",
        "        k: key shape == (..., seq_len_k, depth)\n",
        "        v: value shape == (..., seq_len_v, depth_v)\n",
        "        mask: Float tensor with shape broadcastable \n",
        "            to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "    \n",
        "    Returns:\n",
        "        output, attention_weights\n",
        "    \"\"\"\n",
        "    \n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "    \n",
        "    # scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    \"\"\"\n",
        "    maskê°€ ìˆì„ ê²½ìš° maskingëœ ìë¦¬(mask=1)ì—ëŠ” (-inf)ì— í•´ë‹¹í•˜ëŠ” ì ˆëŒ“ê°’ì´ í° ìŒìˆ˜ -1e9(=-10ì–µ)ì„ ë”í•´ì¤ë‹ˆë‹¤.\n",
        "    ê·¸ ê°’ì— softmaxë¥¼ ì·¨í•´ì£¼ë©´ ê±°ì˜ 0ì— ê°€ê¹Œìš´ ê°’ì´ ë‚˜ì˜µë‹ˆë‹¤. ê·¸ ë‹¤ìŒ value ê³„ì‚°ì‹œì— ë°˜ì˜ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "    \n",
        "    # add the mask to the scaled tensor.\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)  \n",
        "        \n",
        "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "    # add up to 1.\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "    \n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "    \n",
        "    return output, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MultiHead Attention"
      ],
      "metadata": {
        "id": "3RsJgJm-Hl6T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lecture Noteì—ì„œëŠ” kerasì—ì„œ ì œê³µí•˜ëŠ” MultiHead Attentionì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ”, Multi Head Attentionì„ ì§ì ‘ êµ¬í˜„í•´ë³´ê² ìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "l6cn4jmaI5KA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ë¬¸ì œ2"
      ],
      "metadata": {
        "id": "-RAQOH2eHiO8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`\"\"\"input your code\"\"\"` ë¶€ë¶„ì— ì•Œë§ì€ ì½”ë“œë¡œ ì•Œë§ê²Œ ì§ì§€ì–´ì§„ ê²ƒì„ ê³ ë¥´ì‹œì˜¤. (A, B)"
      ],
      "metadata": {
        "id": "r2nXr_0jJG3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def point_wise_feed_forward_network(**kargs):\n",
        "    \"\"\"\n",
        "    FFNNì„ êµ¬í˜„í•œ ì½”ë“œì…ë‹ˆë‹¤.\n",
        "\n",
        "    Args:\n",
        "        d_model : ëª¨ë¸ì˜ ì°¨ì›ì…ë‹ˆë‹¤.\n",
        "        dff : ì€ë‹‰ì¸µì˜ ì°¨ì› ìˆ˜ì…ë‹ˆë‹¤. ë…¼ë¬¸ì—ì„œëŠ” 2048ì„ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "    return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(kargs['dff'], activation='relu'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(kargs['d_model'])  # (batch_size, seq_len, d_model)\n",
        "    ])"
      ],
      "metadata": {
        "id": "fsuAwzvwbgb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kargs):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = kargs['num_heads']\n",
        "        self.d_model = kargs['d_model']\n",
        "\n",
        "        assert self.d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = self.d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(units=self.d_model)\n",
        "        self.wk = tf.keras.layers.Dense(units=self.d_model)\n",
        "        self.wv = tf.keras.layers.Dense(units=self.d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(kargs['d_model'])\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"\n",
        "        Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention, \n",
        "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        return output, attention_weights"
      ],
      "metadata": {
        "id": "i1fiUBlybbQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder Layer"
      ],
      "metadata": {
        "id": "7IOF-NkAKOl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kargs):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(**kargs)\n",
        "        self.ffn = point_wise_feed_forward_network(**kargs)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(kargs['rate'])\n",
        "        self.dropout2 = tf.keras.layers.Dropout(kargs['rate'])\n",
        "\n",
        "    def call(self, x, mask):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        return out2"
      ],
      "metadata": {
        "id": "ZY6JRIMYcAtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder"
      ],
      "metadata": {
        "id": "QpMskzgBKR8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = kwargs['d_model']\n",
        "        self.num_layers = kwargs['num_layers']\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(kwargs['vocab_size'], self.d_model)\n",
        "        self.pos_encoding = positional_encoding(kwargs['maximum_position_encoding'], \n",
        "                                                self.d_model)\n",
        "\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(**kwargs) \n",
        "                           for _ in range(self.num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(kwargs['rate'])\n",
        "\n",
        "    def call(self, x, mask):\n",
        "\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        # adding embedding and position encoding.\n",
        "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, mask)\n",
        "\n",
        "        return x  # (batch_size, input_seq_len, d_model)"
      ],
      "metadata": {
        "id": "6bm0eI4FcDFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer"
      ],
      "metadata": {
        "id": "TbFDQKh6MCT9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì—¬ê¸°ì„œëŠ” ê°€ì¥ ì²« ë²ˆì§¸ íƒ€ì„ ìŠ¤í…ì˜ ì€ë‹‰ ê°’ì„ ê°€ì ¸ì™€ì„œ classification_layerì— í†µê³¼ì‹œí‚¤ê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "(batch_size, 1, d_model) í¬ê¸°ì˜ í…ì„œê°€ ë°”ë¡œ ì´ì§„ ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•˜ëŠ” Dense Layerë¡œ ë“¤ì–´ê°€ê²Œë” êµ¬í˜„í–ˆìŠµë‹ˆë‹¤.\n",
        "- ì´ ë¶€ë¶„ì€ ì„¤ê³„ì— ë”°ë¼ ë³€ê²½ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì´ 2ê°œì˜ Dense Layerë¥¼ ìŒ“ì•„ì„œ ì´ì§„ ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n",
        "  - Dense(64, activation='relu')\n",
        "  - Dense(1, activation='sigmoid')\n"
      ],
      "metadata": {
        "id": "Pnb6bL72M1Yx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, **kargs):\n",
        "        super(Transformer, self).__init__()\n",
        "        \n",
        "        self.encoder = Encoder(**kargs)\n",
        "        self.classification_layer = tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        # Keras models prefer if you pass all your inputs in the first argument\n",
        "\n",
        "        enc_padding_mask = self.create_masks(inputs)\n",
        "\n",
        "        enc_output = self.encoder(inputs, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "\n",
        "        final_output = self.classification_layer(enc_output[:, 0, :]) # (bs, 1, d_model)\n",
        "\n",
        "        return final_output\n",
        "\n",
        "    def create_masks(self, inputs):\n",
        "        # Encoder padding mask\n",
        "        enc_padding_mask = create_padding_mask(inputs)\n",
        "\n",
        "        return enc_padding_mask"
      ],
      "metadata": {
        "id": "_LjmMJg-wiqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyper-Parameter ì„¤ì •"
      ],
      "metadata": {
        "id": "89RBkU8iMEzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 16\n",
        "MAX_SEQUENCE = 80\n",
        "EPOCHS = 3\n",
        "VALID_SPLIT = 0.1\n",
        "MAX_FEATURES = 20000\n",
        "MAXLEN = 80\n",
        "\n",
        "kargs = {'num_layers': 2,\n",
        "         'd_model': 512,\n",
        "         'num_heads': 8,\n",
        "         'dff': 2048,\n",
        "         'vocab_size': MAX_FEATURES,\n",
        "         'maximum_position_encoding': MAXLEN,\n",
        "         'rate': 0.1\n",
        "        }"
      ],
      "metadata": {
        "id": "yBQGLAWnvQpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMDB Review Data ë¶ˆëŸ¬ì˜¤ê¸°"
      ],
      "metadata": {
        "id": "fEdeHPWzMIuf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "# ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=MAX_FEATURES)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqLwHSR-tkZs",
        "outputId": "c242669f-0cf6-4726-90dc-787f399ec6d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=MAXLEN)\n",
        "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=MAXLEN)"
      ],
      "metadata": {
        "id": "q4w_U2Ajtt_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ëª¨ë¸ ì„ ì–¸"
      ],
      "metadata": {
        "id": "tegtzKWSMNT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Transformer(**kargs)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics='accuracy')"
      ],
      "metadata": {
        "id": "_n4QWMruuyO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=10)\n",
        "\n",
        "checkpoint_path = 'weights.h5'\n",
        "\n",
        "cp_callback = ModelCheckpoint(\n",
        "    checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True)"
      ],
      "metadata": {
        "id": "1oaHP-TVtSCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ëª¨ë¸ í•™ìŠµ"
      ],
      "metadata": {
        "id": "fgYE15nHRulT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train, \n",
        "                    batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
        "                    validation_split=VALID_SPLIT, callbacks=[earlystop_callback, cp_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lvz4wIJtuhv9",
        "outputId": "719e2443-287d-4f16-e3a4-f3c0c6b3bb6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "1406/1407 [============================>.] - ETA: 0s - loss: 0.5167 - accuracy: 0.7301\n",
            "Epoch 1: val_accuracy improved from -inf to 0.82680, saving model to weights.h5\n",
            "1407/1407 [==============================] - 49s 30ms/step - loss: 0.5166 - accuracy: 0.7301 - val_loss: 0.3757 - val_accuracy: 0.8268\n",
            "Epoch 2/3\n",
            "1405/1407 [============================>.] - ETA: 0s - loss: 0.3005 - accuracy: 0.8725\n",
            "Epoch 2: val_accuracy improved from 0.82680 to 0.83080, saving model to weights.h5\n",
            "1407/1407 [==============================] - 42s 30ms/step - loss: 0.3005 - accuracy: 0.8724 - val_loss: 0.3849 - val_accuracy: 0.8308\n",
            "Epoch 3/3\n",
            "1405/1407 [============================>.] - ETA: 0s - loss: 0.1719 - accuracy: 0.9348\n",
            "Epoch 3: val_accuracy did not improve from 0.83080\n",
            "1407/1407 [==============================] - 41s 29ms/step - loss: 0.1719 - accuracy: 0.9348 - val_loss: 0.4611 - val_accuracy: 0.8132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0JyD-x3Ix_Rd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "n424a_Transformer_and_BERT_GPT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}